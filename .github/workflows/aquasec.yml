name: KubeEnforcer / PodEnforcer on EKS with Fargate profile

on:
  push:
    branches: [ "main" ]

env:
  AWS_REGION:   us-west-1                   
  EKS_CLUSTER:  AndreasMCluster
  NAMESPACE:    aqua

permissions:
  contents: read

jobs:
  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    environment: production

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: installing eksctl
      run:  |
            curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
            sudo mv /tmp/eksctl /usr/local/bin
            eksctl version

    - name: Check if the cluster already exists
      id: ClusterExists
      run: echo "result=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "contains(clusters,'${{ env.EKS_CLUSTER }}')")" >> $GITHUB_OUTPUT          
     
### Wenn Cluster noch nicht existiert 
### sa https://docs.aws.amazon.com/eks/latest/userguide/fargate.html
    - name: create an EKS cluster with Fargate profile
      if: steps.ClusterExists.outputs.result == 'false'
      run:  |
            eksctl create cluster \
            --name ${{ env.EKS_CLUSTER }} \
            --region ${{ env.AWS_REGION }} \
            --fargate --version 1.27

    - name: Check if the cluster exists
      id: ClusterExists2
      run: echo "result=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "contains(clusters,'${{ env.EKS_CLUSTER }}')")" >> $GITHUB_OUTPUT          
     
    - name: configure kubectl
      if: steps.ClusterExists2.outputs.result == 'true'
      run:  |
            aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}
            kubectl version --client=true -o json | jq '.clientVersion.gitVersion' -r 
            kubectl get nodes -o wide
            kubectl get pods -A -o wide

# ### In Fargate the fargate-profile needs to be created first. Once this is created, the namespace can be created, and everything works.
    # - name: create namespace for Aqua with fargate profile   
      # if: steps.ClusterExists2.outputs.result == 'true'
      # run:  |
            # eksctl create fargateprofile \
                # --region ${{ env.AWS_REGION }} \
                # --cluster ${{ env.EKS_CLUSTER }} \
                # --name aqua-fargate-profile \
                # --namespace ${{ env.NAMESPACE }}

            # kubectl create ns ${{ env.NAMESPACE }}

    - name: deploy Aqua with helm
      if: steps.ClusterExists2.outputs.result == 'true'
      run:  |
            helm repo add aqua-helm https://helm.aquasec.com
            helm repo update
            helm search repo aqua-helm
            helm upgrade --install --namespace ${{ env.NAMESPACE }} kube-enforcer aqua-helm/kube-enforcer --version 2022.4 -f ./kube-enforcer-values.yaml --set imageCredentials.username=${{ secrets.AQUA_USER }},imageCredentials.password=${{ secrets.AQUA_PASSWORD }}
            
    - name: deploy some workload   
      if: steps.ClusterExists2.outputs.result == 'true'
      run:  |
            kubectl create deployment netshoot --image=nicolaka/netshoot -- "/bin/sleep" "infinity"

    - name: Utilities
      run: |
          echo CloudFormation:
          echo "https://us-west-1.console.aws.amazon.com/cloudformation/home?region=us-west-1#/stacks?filteringText=&filteringStatus=active&viewNested=true"
          echo
          echo "aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}"
          echo
          echo "eksctl delete cluster --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}"
